from __future__ import annotations

import os
import re
from typing import List
import json
from typing import Any
from ollama import chat
from dotenv import load_dotenv

load_dotenv()

BULLET_PREFIX_PATTERN = re.compile(r"^\s*([-*â€¢]|\d+\.)\s+")
KEYWORD_PREFIXES = (
    "todo:",
    "action:",
    "next:",
)


def _is_action_line(line: str) -> bool:
    stripped = line.strip().lower()
    if not stripped:
        return False
    if BULLET_PREFIX_PATTERN.match(stripped):
        return True
    if any(stripped.startswith(prefix) for prefix in KEYWORD_PREFIXES):
        return True
    if "[ ]" in stripped or "[todo]" in stripped:
        return True
    return False


def extract_action_items(text: str) -> List[str]:
    lines = text.splitlines()
    extracted: List[str] = []
    for raw_line in lines:
        line = raw_line.strip()
        if not line:
            continue
        if _is_action_line(line):
            cleaned = BULLET_PREFIX_PATTERN.sub("", line)
            cleaned = cleaned.strip()
            # Trim common checkbox markers
            cleaned = cleaned.removeprefix("[ ]").strip()
            cleaned = cleaned.removeprefix("[todo]").strip()
            extracted.append(cleaned)
    # Fallback: if nothing matched, heuristically split into sentences and pick imperative-like ones
    if not extracted:
        sentences = re.split(r"(?<=[.!?])\s+", text.strip())
        for sentence in sentences:
            s = sentence.strip()
            if not s:
                continue
            if _looks_imperative(s):
                extracted.append(s)
    # Deduplicate while preserving order
    seen: set[str] = set()
    unique: List[str] = []
    for item in extracted:
        lowered = item.lower()
        if lowered in seen:
            continue
        seen.add(lowered)
        unique.append(item)
    return unique


def _looks_imperative(sentence: str) -> bool:
    words = re.findall(r"[A-Za-z']+", sentence)
    if not words:
        return False
    first = words[0]
    # Crude heuristic: treat these as imperative starters
    imperative_starters = {
        "add",
        "create",
        "implement",
        "fix",
        "update",
        "write",
        "check",
        "verify",
        "refactor",
        "document",
        "design",
        "investigate",
    }
    return first.lower() in imperative_starters

# Generated by Cursor AI - TODO 1: LLM-powered extraction
def extract_action_items_llm(text: str) -> List[str]:
    """
    Use a local Ollama model to extract actionable tasks from free-form text.

    The LLM is instructed to return ONLY a JSON array of strings. This
    function validates and sanitizes the response, handling JSON parsing
    errors and edge cases like empty input or empty items.
    """
    normalized = (text or "").strip()
    if not normalized:
        return []

    system_prompt = (
        "You extract actionable tasks from text.\n"
        "Return ONLY a valid JSON array of strings.\n"
        "Each string must be a concise, actionable task.\n"
        "If there are no actionable tasks, return an empty JSON array [].\n"
        "Do not include any explanation or extra text."
    )

    model_name = os.getenv("OLLAMA_MODEL", "llama3.2")

    try:
        response: Any = chat(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": normalized},
            ],
        )
    except Exception:
        # If the LLM call fails entirely, fall back to the heuristic extractor.
        return extract_action_items(normalized)

    content = ""
    if isinstance(response, dict):
        message = response.get("message") or {}
        if isinstance(message, dict):
            content = (message.get("content") or "").strip()
    if not content:
        return extract_action_items(normalized)

    def _parse_json_array(raw: str) -> List[str] | None:
        # First attempt: direct JSON parse
        try:
            data = json.loads(raw)
        except json.JSONDecodeError:
            # Handle common formatting issues like Markdown code fences
            cleaned = raw.strip()
            if cleaned.startswith("```"):
                cleaned = re.sub(r"^```(?:json)?\s*", "", cleaned)
                cleaned = re.sub(r"\s*```$", "", cleaned)
            # Try to extract the first JSON array substring
            start = cleaned.find("[")
            end = cleaned.rfind("]")
            if start != -1 and end != -1 and end > start:
                snippet = cleaned[start : end + 1]
                try:
                    data = json.loads(snippet)
                except json.JSONDecodeError:
                    return None
            else:
                return None

        # Normalize to a list of strings
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            for key in ("tasks", "action_items", "actions"):
                value = data.get(key)
                if isinstance(value, list):
                    items = value
                    break
            else:
                return None
        else:
            return None

        cleaned_items: List[str] = []
        for item in items:
            if not isinstance(item, str):
                continue
            s = item.strip()
            if s:
                cleaned_items.append(s)
        return cleaned_items

    parsed = _parse_json_array(content)
    if not parsed:
        # If parsing failed, fall back to heuristic extraction
        return extract_action_items(normalized)

    # Deduplicate while preserving order
    seen: set[str] = set()
    unique: List[str] = []
    for item in parsed:
        lowered = item.lower()
        if lowered in seen:
            continue
        seen.add(lowered)
        unique.append(item)
    return unique
